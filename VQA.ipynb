{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list that specifies convolution-pooling architecture\n",
    "# list index indicate layer position in stack; \n",
    "# a pooling layer is represented by a tuple: (pooling type, kernel_size, strides)\n",
    "# a convolution layer is represented by a typle: (filter_height, filter_width, depth)\n",
    "layers = [(5, 5, 6),\n",
    "          ('MAX', (1,2,2,1), (1,2,2,1)),\n",
    "          (5, 5, 16),\n",
    "          ('MAX', (1,2,2,1), (1,2,2,1)),\n",
    "          (5, 5, 60),\n",
    "          ('MAX', (1,2,2,1), (1,2,2,1))]\n",
    "\n",
    "ResNet_block_layers = [(1, 1, 128, 'relu'),\n",
    "                       (3, 3, 128, 'relu'),\n",
    "                       (1, 1, 512, None)]\n",
    "inception_depths = [64, (96, 128), (16, 32), 32]\n",
    "\n",
    "def conv_pool(x, layers):\n",
    "    out = x\n",
    "    n_conv, n_pool = 0, 0\n",
    "    prev_depth = int(x.shape[3])\n",
    "    for l in layers:\n",
    "        if type(l[0]) == int:\n",
    "            n_conv += 1\n",
    "            with tf.variable_scope('conv_{}'.format(n_conv), reuse = tf.AUTO_REUSE):\n",
    "                w = tf.get_variable('filter', initializer=tf.truncated_normal((l[0],l[1],prev_depth,l[2]),0,0.1))\n",
    "                b = tf.get_variable('bias', initializer=tf.zeros(l[2]))\n",
    "            out = tf.nn.relu(tf.nn.conv2d(out, w, strides=(1,1,1,1), padding='SAME') + b)\n",
    "            prev_depth = l[2]\n",
    "        else:\n",
    "            n_pool += 1\n",
    "            out = tf.nn.pool(out, pooling_type=l[0], window_shape=l[1], strides=l[2],\n",
    "                             padding='SAME', name='pool_{}'.format(n_pool))\n",
    "    return out\n",
    "\n",
    "def ResNet_block(x, layers, name):\n",
    "    out = x\n",
    "    n_conv = 0\n",
    "    if int(x.shape[3]) != layers[-1][2]:\n",
    "        print('Input to ResNet block must have the same shape as output of convolution layers')\n",
    "        return\n",
    "    prev_depth = int(x.shape[3])\n",
    "    with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "        for l in layers:\n",
    "            with tf.variable_scope('conv_'.format(n_conv), reuse = tf.AUTO_REUSE):\n",
    "                n_conv += 1\n",
    "                w = tf.get_variable('filter', initializer=tf.truncated_normal((l[0],l[1],prev_depth,l[2]),0,0.1))\n",
    "                b = tf.get_variable('bias', initializer=tf.zeros(l[2]))\n",
    "            out = tf.nn.conv2d(out, w, strides=(1,1,1,1), padding='SAME') + b\n",
    "            if l[3] == 'relu':\n",
    "                out = tf.nn.relu(out)\n",
    "            prev_depth = l[2]\n",
    "    return tf.nn.relu(out + x)\n",
    "\n",
    "\n",
    "def Inception_module(x, depths):\n",
    "    layers = [[(1, 1, depths[0])]\n",
    "              [(1, 1, depths[1][0]), (3, 3, depths[1][1])],\n",
    "              [(1, 1, depths[2][0]), (5, 5, depths[2][1])], \n",
    "              [('MAX', (1,3,3,1), (1,1,1,1)), (1, 1, depths[3])]]\n",
    "    out = []\n",
    "    for i in range(4):\n",
    "         with tf.variable_scope('component_{}'.format(i+1), reuse = tf.AUTO_REUSE):\n",
    "                out.append(conv_pool(x, layers[i]))\n",
    "    return tf.concat(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all frames from video downscaled by a factor\\n\"\n",
    "# return an ndarray of shape (n_frames, height, width, channels)\n",
    "\n",
    "def get_frames(path, n_frames, downscale_factor):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    seq = []\n",
    "    count = 0\n",
    "    while True:\n",
    "        success,frame = cap.read()\n",
    "        if count == n_frames or not success:\n",
    "            break\n",
    "        # downscale frame\n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        resized = cv2.resize(frame, (width, height), interpolation = cv2.INTER_AREA)\n",
    "        if resized.shape[0] > resized.shape[1]:\n",
    "            resized = np.transpose(resized, (1,0,2))\n",
    "        seq.append(resized)\n",
    "        count += 1\n",
    "    return np.stack(seq)\n",
    "\n",
    "# mini-batch generator\n",
    "def next_batch(path, labels, n_batches, batch_size, n_frames, downscale_factor):\n",
    "    perm = np.random.permutation(300)\n",
    "    for i in range(n_batches):\n",
    "        x_batch, y_batch = [], []\n",
    "        for j in range(0, batch_size):\n",
    "            all_frames = get_frames(path.format(perm[i*batch_size+j]+1), n_frames, downscale_factor)\n",
    "            #print(all_frames.shape)\n",
    "            x_batch.append(all_frames)\n",
    "            y_batch.append(labels[perm[i*batch_size+j]])\n",
    "        x_batch = np.stack(x_batch)\n",
    "        yield x_batch, y_batch\n",
    "            \n",
    "# generate feature maps for each video in mini-batch\n",
    "# x has shape (batch_size, n_frames, height, width, channels)\n",
    "def get_feature_maps(x):\n",
    "    instances = []\n",
    "    for i in range(x.shape[0]):\n",
    "        instances.append(tf.contrib.layers.flatten(conv_pool(x[i, :, :, :, :], layers)))\n",
    "    return tf.stack(instances, axis=0)\n",
    "\n",
    "def score_to_label(scores, thresh_1, thresh_2):\n",
    "    for x in np.nditer(scores, op_flags=['readwrite']):\n",
    "        if x < thresh_1:\n",
    "            x[...] = 0\n",
    "        elif x < thresh_2:\n",
    "            x[...] = 1\n",
    "        else:\n",
    "            x[...] = 2\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/mallesh/video-qoe-labeling_1/VQA-Deep-Learning/data/set1-4/trace_{}.mp4'\n",
    "height, width, n_channels = 1080, 1920, 3\n",
    "downscale_factor = 8\n",
    "n_frames = 100\n",
    "n_classes = 3\n",
    "n_batches, batch_size = 30, 10\n",
    "n_hidden = 100 # number of hidden cells in LSTM\\n\"\n",
    "X = tf.placeholder(tf.float32, shape=\n",
    "                   (batch_size, n_frames, int(height/downscale_factor), int(width/downscale_factor), n_channels))\n",
    "y = tf.placeholder(tf.int32, shape=(batch_size,))\n",
    "labels = score_to_label(np.loadtxt('/home/mallesh/video-qoe-labeling_1/VQA-Deep-Learning/data/set1-4.txt'), 2, 3.8)\n",
    "X_features = get_feature_maps(X)\n",
    "print(X_features.shape)\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "output, _ = tf.nn.dynamic_rnn(cell, X_features, initial_state = cell.zero_state(batch_size, dtype=tf.float32))\n",
    "with tf.variable_scope('out', reuse = tf.AUTO_REUSE):\n",
    "    w = tf.get_variable('weight', shape=(n_hidden, n_classes))\n",
    "    b = tf.get_variable('bias', initializer=tf.zeros(n_classes))\n",
    "    pred = tf.matmul(output[:,-1,:], w) + b\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    loss_summary = tf.summary.scalar('loss', loss)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_num = 0\n",
    "    for X_batch, y_batch in next_batch(path, labels, n_batches, batch_size, n_frames, downscale_factor):\n",
    "        print(X_batch.shape)\n",
    "        batch_num += 1\n",
    "        summary_str = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        file_writer.add_summary(summary_str, batch_num)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        saver.save(sess, '/tmp/after_batch_{}.ckpt'.format(batch_num))\n",
    "        print(pred.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        print(loss.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        saver.save(sess, '/tmp/final.ckpt')\n",
    "        file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
